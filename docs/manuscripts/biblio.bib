@article{wilkinson_2016,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, IJsbrand Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J.G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A.C and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  date = {2016-12},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {3},
  pages = {160018},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  url = {http://www.nature.com/articles/sdata201618},
  urldate = {2020-06-06},
  file = {/home/cantabile/Zotero/storage/YCFHF733/Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data ma.pdf},
  langid = {english},
  number = {1},
  options = {useprefix=true}
}

@article{laine_2018,
  title = {Open Science and Codes of Conduct on Research Integrity},
  author = {Laine, Heidi},
  date = {2018-12-31},
  journaltitle = {Informaatiotutkimus},
  shortjournal = {1},
  volume = {37},
  issn = {1797-9129},
  doi = {10.23978/inf.77414},
  url = {https://journal.fi/inf/article/view/77414},
  urldate = {2019-08-23},
  file = {/home/cantabile/Zotero/storage/UA3HYS9T/Laine - 2018 - Open science and codes of conduct on research inte.pdf;/home/cantabile/Zotero/storage/5GEZCWK2/77414.html},
  langid = {english},
  number = {4}
}

@article{peng_reproducible_2011-1,
  title = {Reproducible {{Research}} in {{Computational Science}}},
  author = {Peng, R. D.},
  date = {2011-12},
  journaltitle = {Science},
  volume = {334},
  pages = {1226--1227},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1213847},
  file = {/home/cantabile/Zotero/storage/IZ8QGLMI/Peng - 2011 - Reproducible Research in Computational Science.pdf},
  number = {6060}
}

@article{collaboration_2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {Collaboration, Open Science},
  date = {2015-08-28},
  journaltitle = {Science},
  volume = {349},
  number = {6251},
  eprint = {26315443},
  eprinttype = {pmid},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  url = {https://science.sciencemag.org/content/349/6251/aac4716},
  urldate = {2020-01-03},
  abstract = {Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 Structured Abstract INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
  langid = {english},
  file = {/home/cantabile/Zotero/storage/ZD4BHQ8K/Collaboration - 2015 - Estimating the reproducibility of psychological sc.pdf;/home/cantabile/Zotero/storage/QIJX9YS8/aac4716.html}
}

@article{devezer_2019,
  title = {Scientific Discovery in a Model-Centric Framework: {{Reproducibility}}, Innovation, and Epistemic Diversity},
  shorttitle = {Scientific Discovery in a Model-Centric Framework},
  author = {Devezer, Berna and Nardin, Luis G. and Baumgaertner, Bert and Buzbas, Erkan Ozge},
  date = {2019-05-15},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {14},
  number = {5},
  pages = {e0216125},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0216125},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0216125},
  urldate = {2021-07-23},
  abstract = {Consistent confirmations obtained independently of each other lend credibility to a scientific result. We refer to results satisfying this consistency as reproducible and assume that reproducibility is a desirable property of scientific discovery. Yet seemingly science also progresses despite irreproducible results, indicating that the relationship between reproducibility and other desirable properties of scientific discovery is not well understood. These properties include early discovery of truth, persistence on truth once it is discovered, and time spent on truth in a long-term scientific inquiry. We build a mathematical model of scientific discovery that presents a viable framework to study its desirable properties including reproducibility. In this framework, we assume that scientists adopt a model-centric approach to discover the true model generating data in a stochastic process of scientific discovery. We analyze the properties of this process using Markov chain theory, Monte Carlo methods, and agent-based modeling. We show that the scientific process may not converge to truth even if scientific results are reproducible and that irreproducible results do not necessarily imply untrue results. The proportion of different research strategies represented in the scientific population, scientists’ choice of methodology, the complexity of truth, and the strength of signal contribute to this counter-intuitive finding. Important insights include that innovative research speeds up the discovery of scientific truth by facilitating the exploration of model space and epistemic diversity optimizes across desirable properties of scientific discovery.},
  langid = {english},
  keywords = {Markov models,Replication studies,Reproducibility,Scientists,Species diversity,Statistical data,Statistical theories,Stochastic processes},
  file = {/home/cantabile/Zotero/storage/GR8QW8GT/Devezer et al. - 2019 - Scientific discovery in a model-centric framework.pdf;/home/cantabile/Zotero/storage/AWBJ2I9H/article.html}
}

@article{szollosi_2019b,
  title = {Is Preregistration Worthwhile?},
  author = {Szollosi, Aba and Kellen, David and Navarro, Danielle and Shiffrin, Richard and van Rooij, Iris and Zandt, Trisha Van and Donkin, Chris},
  date = {2019-10-31T20:58:08},
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/x36pz},
  url = {https://psyarxiv.com/x36pz/},
  urldate = {2021-07-23},
  abstract = {Proponents of preregistration argue that, among other benefits, it improves the diagnosticity of statistical tests [1]. In the strong version of this argument, preregistration does this by solving statistical problems, such as family-wise error rates. In the weak version, it nudges people to think more deeply about their theories, methods, and analyses. We argue against both: the diagnosticity of statistical tests depend entirely on how well statistical models map onto underlying theories, and so improving statistical techniques does little to improve theories when the mapping is weak. There is also little reason to expect that preregistration will spontaneously help researchers to develop better theories (and, hence, better methods and analyses).},
  keywords = {Meta-science,metascience,preregistration,Social and Behavioral Sciences,theory,Theory and Philosophy of Science},
  file = {/home/cantabile/Zotero/storage/JLWNK2UH/Szollosi et al. - 2019 - Is preregistration worthwhile.pdf}
}

@article{birkinshaw_2021,
  title = {Antidepressants for Pain Management in Adults with Chronic Pain: A Network Meta‐analysis},
  shorttitle = {Antidepressants for Pain Management in Adults with Chronic Pain},
  author = {Birkinshaw, Hollie and Friedrich, Claire and Cole, Peter and Eccleston, Christopher and Serfaty, Marc and Stewart, Gavin and White, Simon and Moore, R. Andrew and Pincus, Tamar},
  date = {2021},
  journaltitle = {Cochrane Database of Systematic Reviews},
  number = {4},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {1465-1858},
  doi = {10.1002/14651858.CD014682},
  url = {https://www.cochranelibrary.com/cdsr/doi/10.1002/14651858.CD014682/full},
  urldate = {2021-04-25},
  langid = {english},
  file = {/home/cantabile/Zotero/storage/LPVSXYHK/full.html}
}

@article{markowetz_2015,
  title = {Five Selfish Reasons to Work Reproducibly},
  author = {Markowetz, Florian},
  date = {2015-12-08},
  journaltitle = {Genome Biology},
  shortjournal = {Genome Biology},
  volume = {16},
  number = {1},
  pages = {274},
  issn = {1474-760X},
  doi = {10.1186/s13059-015-0850-7},
  url = {https://doi.org/10.1186/s13059-015-0850-7},
  urldate = {2021-07-23},
  abstract = {And so, my fellow scientists: ask not what you can do for reproducibility; ask what reproducibility can do for you! Here, I present five reasons why working reproducibly pays off in the long run and is in the self-interest of every ambitious, career-oriented scientist.},
  keywords = {Reproducibility,Scientific career},
  file = {/home/cantabile/Zotero/storage/JMMFMGG8/Markowetz - 2015 - Five selfish reasons to work reproducibly.pdf;/home/cantabile/Zotero/storage/GLCS96VE/s13059-015-0850-7.html}
}

@article{baggerly_2009,
  title = {Deriving Chemosensitivity from Cell Lines: {{Forensic}} Bioinformatics and Reproducible Research in High-Throughput Biology},
  shorttitle = {Deriving Chemosensitivity from Cell Lines},
  author = {Baggerly, Keith A. and Coombes, Kevin R.},
  date = {2009-12},
  journaltitle = {The Annals of Applied Statistics},
  volume = {3},
  number = {4},
  pages = {1309--1334},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1932-6157, 1941-7330},
  doi = {10.1214/09-AOAS291},
  url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-3/issue-4/Deriving-chemosensitivity-from-cell-lines--Forensic-bioinformatics-and-reproducible/10.1214/09-AOAS291.full},
  urldate = {2021-07-25},
  abstract = {High-throughput biological assays such as microarrays let us ask very detailed questions about how diseases operate, and promise to let us personalize therapy. Data processing, however, is often not described well enough to allow for exact reproduction of the results, leading to exercises in “forensic bioinformatics” where aspects of raw data and reported results are used to infer what methods must have been employed. Unfortunately, poor documentation can shift from an inconvenience to an active danger when it obscures not just methods but errors. In this report we examine several related papers purporting to use microarray-based signatures of drug sensitivity derived from cell lines to predict patient response. Patients in clinical trials are currently being allocated to treatment arms on the basis of these results. However, we show in five case studies that the results incorporate several simple errors that may be putting patients at risk. One theme that emerges is that the most common errors are simple (e.g., row or column offsets); conversely, it is our experience that the most simple errors are common. We then discuss steps we are taking to avoid such errors in our own investigations.},
  keywords = {forensic bioinformatics,microarrays,reproducibility},
  file = {/home/cantabile/Zotero/storage/4PC9QHFH/Baggerly and Coombes - 2009 - Deriving chemosensitivity from cell lines Forensi.pdf;/home/cantabile/Zotero/storage/ILP39GDQ/09-AOAS291.html}
}

@online{schleunes_2020,
  title = {Top {{Spider Biologist}}’s {{Research Under Fire}}},
  author = {Schleunes, Amy},
  date = {2020},
  url = {https://www.the-scientist.com/news-opinion/top-spider-biologists-research-under-fire-67083},
  urldate = {2021-07-25},
  abstract = {After the initial announcements of two retractions, scientists have mobilized to interrogate the data in nearly 150 of Jonathan Pruitt's papers.},
  langid = {english},
  organization = {{The Scientist Magazine®}},
  file = {/home/cantabile/Zotero/storage/65INSM3V/top-spider-biologists-research-under-fire-67083.html}
}

@article{fraser_questionable_2018,
  title = {Questionable Research Practices in Ecology and Evolution},
  author = {Fraser, Hannah and Parker, Tim and Nakagawa, Shinichi and Barnett, Ashley and Fidler, Fiona},
  date = {2018-07-16},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {13},
  number = {7},
  pages = {e0200303},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0200303},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0200303},
  urldate = {2019-06-27},
  abstract = {We surveyed 807 researchers (494 ecologists and 313 evolutionary biologists) about their use of Questionable Research Practices (QRPs), including cherry picking statistically significant results, p hacking, and hypothesising after the results are known (HARKing). We also asked them to estimate the proportion of their colleagues that use each of these QRPs. Several of the QRPs were prevalent within the ecology and evolution research community. Across the two groups, we found 64\% of surveyed researchers reported they had at least once failed to report results because they were not statistically significant (cherry picking); 42\% had collected more data after inspecting whether results were statistically significant (a form of p hacking) and 51\% had reported an unexpected finding as though it had been hypothesised from the start (HARKing). Such practices have been directly implicated in the low rates of reproducible results uncovered by recent large scale replication studies in psychology and other disciplines. The rates of QRPs found in this study are comparable with the rates seen in psychology, indicating that the reproducibility problems discovered in psychology are also likely to be present in ecology and evolution.},
  langid = {english},
  keywords = {Behavioral ecology,Community ecology,Ecology and environmental sciences,Evolutionary biology,Evolutionary ecology,Psychology,Publication ethics,Statistical data},
  file = {/home/cantabile/Zotero/storage/AAKNA8XE/Fraser et al. - 2018 - Questionable research practices in ecology and evo.pdf;/home/cantabile/Zotero/storage/RQB73HN3/article.html}
}

@book{landau_2021,
  title = {The Targets {{R Package User Manual}}},
  author = {Landau, Will},
  date = {2021},
  url = {https://books.ropensci.org/targets},
  urldate = {2021-07-14},
  abstract = {In-depth discussion of the major functionality of targets.},
  file = {/home/cantabile/Zotero/storage/HDAXL4II/targets.html}
}

@manual{bryan_2021,
  type = {manual},
  title = {Googlesheets4: {{Access}} Google Sheets Using the Sheets {{API V4}}},
  author = {Bryan, Jennifer},
  date = {2021},
  url = {https://CRAN.R-project.org/package=googlesheets4}
}
